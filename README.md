# Setup Instructions

## ğŸ› ï¸ Required Software

To run this project, please ensure you have the following software installed:

| Software | Version  | Installation Instructions |
|----------|----------|----------------------------|
| Python   | 3.10     | [Install Python 3.10](https://www.python.org/downloads/release/python-3100/) |
| SWI-Prolog | 9.2.9 | [Install SWI-Prolog 9.2.9](https://www.swi-prolog.org/Download.html) |

> â„¹ï¸ **Note:** It's recommended to use tools like `pyenv` or `conda` for managing multiple Python versions, and to verify Prolog installation with `swipl --version`.

## ğŸ§ª Running Experiments

### ğŸ“¦ Experiment Setup

To configure the environment necessary for running all experiments, execute the following commands from the root directory:

```bash
cd code/experiments
pip install -r requirements.txt
```

This will install all Python dependencies required across the experiment modules.

---

### ğŸš€ Experiment Execution

All experiment commands should be run from within the `code/experiments` directory.

#### ğŸ”¹ Experiment 1: BLEU and Bert Score Evaluation

```bash
cd bleu_experiments
python experiment.py
```

This script initiates the BLEU-based evaluation pipeline, processing predefined prompts against system generated prompts to compute comparative BLEU and Bert scores.

---

#### ğŸ”¹ Experiment 2: LLM Evaluation without System Prompt

```bash
cd llm_experiment
python experiment.py no_system_prompt
```

This configuration runs the LLM-based experiment without applying a system-level prompt, simulating a baseline inference setting.

---

#### ğŸ”¹ Experiment 3: LLM Evaluation with System Prompt

```bash
cd llm_experiment
python experiment.py system_prompt
```

This execution variant includes a system prompt to assess prompt engineering effects on LLM response generation and evaluation metrics.

---

## ğŸ§¾ Generating Evaluation Datasets using PromptGen

This project includes two systems for generating evaluation datasets using Prolog-based logic programs. The evaluation sets generated by these two systems are the ones being used in the experiments.

---

### ğŸ§  System 1 Instructions

1. Navigate to the System 1 directory:
   ```bash
   cd code/method1
   ```

2. Run the dataset generation driver using SWI-Prolog:
   ```bash
   swipl -s driver.pl
   ```

3. This will automatically generate text files containing various types of questionâ€“answer (QA) pairs per predicate based on the annotations defined in `annotations.pl`.

---

### ğŸ§  System 2 Instructions

1. Navigate to the System 2 directory:
   ```bash
   cd code/method2
   ```

2. Start the SWI-Prolog interpreter:
   ```bash
   swipl
   ```

3. Inside the Prolog interpreter, load the backend logic:
   ```prolog
   consult("backend.pl").
   ```

4. Then run the generation procedure:
   ```prolog
   generate.
   ```

5. This will create different types of QA pair datasets and save them to their respective files as defined in the `backend.pl` logic.

---